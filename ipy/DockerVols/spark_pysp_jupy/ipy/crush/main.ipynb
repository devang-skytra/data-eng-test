{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"LOCAL JUPYTER TO PY file generation  vs  SPARK DEPLOYMENT \n",
    "\n",
    "Locally has extra steps eg. from google.cloud import bigquery\n",
    "but simply split cells as needed and \n",
    "DO NOT %%writefile\n",
    "\n",
    "start with #%%writefile and then do Edit Find/Replace #%% with %%\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writefile start\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../py/crush/main.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile ../../py/crush/main.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "gcloud dataproc jobs submit pyspark gs://pysparkcode/crush/main.py --cluster=$cls --region=$reg --bucket=dp-staging-forfree \n",
    "# --jars=gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar\n",
    "\n",
    "use '--' to separate arguments to your job from arguments to gcloud:\n",
    "...main.py -- 7 2017-11-01\n",
    "var = int(sys.argv[1]) if len(sys.argv) > 1 else 2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sql,localhost,DataOps,sa,KO9bVGoGAvF8t5H6ldB8', 'sql,localhost,realestate_DEV,sa,KO9bVGoGAvF8t5H6ldB8', 'sql,pddemodw.database.windows.net,dw_admin,pdtest_sa,CX#5-RedRooster', '/media/paul/code/db', '/media/paul/code/etl']\n"
     ]
    }
   ],
   "source": [
    "#%%writefile -a ../../py/crush/main.py\n",
    "\n",
    "import sys, os, json\n",
    "from os.path import dirname, abspath\n",
    "\n",
    "# sys.argv[0] is program fullpath hence can check context ipy vs py\n",
    "if 'ipy' in sys.argv[0]:\n",
    "    f_base = abspath(__name__).replace('__','')\n",
    "    f_json = f_base.replace('ipy','ipy_conf') + '.json'\n",
    "    \n",
    "    with open(f_json, 'r') as config_file:\n",
    "        config_dict = json.load(config_file)\n",
    "    _arg = config_dict[\"configurations\"][0][\"linux\"][\"args\"]\n",
    "else:\n",
    "    _arg = sys.argv\n",
    "\n",
    "# print(_arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writefile end\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import dirname, abspath\n",
    "\n",
    "top = dirname(dirname(dirname(abspath(__name__)))) \n",
    "sys.path.append(os.path.join(top, 'py_modules'))\n",
    "# sys.path.append(os.path.join(top, 'keys'))\n",
    "\n",
    "\n",
    "import bq_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test __init__(self, proj_id, cred_fullpath = None):\n",
    "# proj_id = 'prj-sbe'\n",
    "proj_id = 'forfree-288615'\n",
    "region = 'eu'\n",
    "bq = bq_helper.bqh(\n",
    "        proj_id,\n",
    "        region,\n",
    "        os.path.join(top, 'keys', 'forfree-usvc-dataproc.json')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bq.run_qry('select * from dbt_raw_data.aisles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writefile start\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a ../../py/crush/main.py\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext #, Row\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Crush-BQ-to-PQ\"\n",
    "    ).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "tb = \"eu_aaa_Crush.t_schema_cols\"\n",
    "pj = \"forfree-288615\"\n",
    "df_tb = sqlContext.read.format(\"bigquery\").option(\"table\", tb) \\\n",
    "  .option(\"parentProject\", pj) \\\n",
    "  .option(\"filter\", \"ds = 'eu_austin_bikeshare'\") \\\n",
    "  .load()\n",
    "# df_tb.printSchema()\n",
    "df_tb.write.parquet('gs://pysparkoutput/' + tb.replace('.','/') + '/', mode = 'overwrite')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writefile end\n",
    "--"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
