{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from py_modules import gcs_helper\n",
    "\n",
    "# V = Validation folder\n",
    "gcsV = gcs_helper.gcsh('devandtest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'SELECT yoy_yield, yoy_rpk, yoy_revenue, yoy_net_revenue, yoy_yield_7day, yoy_rpk'\n",
      "b'SELECT yoy_yield_7day, yoy_rpk_7day, yoy_revenue_7day, yoy_net_revenue_7day, yoy'\n",
      "b'SELECT cumulative_yoy_yield, cumulative_yoy_rpk, cumulative_yoy_revenue, cumulat'\n",
      "b'SELECT yoy_yield_7day, yoy_rpk_7day, yoy_revenue_7day, yoy_net_revenue_7day, cum'\n",
      "b'SELECT cumulative_yoy_yield, cumulative_yoy_rpk, cumulative_yoy_revenue, cumulat'\n",
      "b'SELECT yoy_yield, yoy_rpk, yoy_revenue, yoy_net_revenue, yoy_yield_7day, yoy_rpk'\n"
     ]
    }
   ],
   "source": [
    "bkt = 'samartest'\n",
    "fld = 'templates/validation'\n",
    "\n",
    "P021 = '0' \n",
    "P001 = '1'\n",
    "P019 = '2' \n",
    "P022 = '3' \n",
    "P020 = '4' \n",
    "P002 = '5' \n",
    "\n",
    "# squarepoint\n",
    "cust_sqp = '26'\n",
    "\n",
    "fle01 = f'{fld}/{cust_sqp}-{P001}.txt'\n",
    "fle02 = f'{fld}/{cust_sqp}-{P002}.txt'\n",
    "fle19 = f'{fld}/{cust_sqp}-{P019}.txt'\n",
    "fle20 = f'{fld}/{cust_sqp}-{P020}.txt'\n",
    "fle21 = f'{fld}/{cust_sqp}-{P021}.txt'\n",
    "fle22 = f'{fld}/{cust_sqp}-{P022}.txt'\n",
    "\n",
    "sql01 = gcsV.file_get(bkt, fle01, False)\n",
    "sql02 = gcsV.file_get(bkt, fle02, False)\n",
    "sql19 = gcsV.file_get(bkt, fle19, False)\n",
    "sql20 = gcsV.file_get(bkt, fle20, False)\n",
    "sql21 = gcsV.file_get(bkt, fle21, False)\n",
    "sql22 = gcsV.file_get(bkt, fle22, False)\n",
    "\n",
    "print_len = 80\n",
    "\n",
    "print(sql01[:print_len])\n",
    "print(sql02[:print_len])\n",
    "print(sql19[:print_len])\n",
    "print(sql20[:print_len])\n",
    "print(sql21[:print_len])\n",
    "print(sql22[:print_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_len = 2000\n",
    "\n",
    "# print(sql01.decode()[:print_len] + '\\n\\n')\n",
    "# print(sql02.decode()[:print_len] + '\\n\\n')\n",
    "# print(sql19.decode()[:print_len] + '\\n\\n')\n",
    "# print(sql20.decode()[:print_len] + '\\n\\n')\n",
    "# print(sql21.decode()[:print_len] + '\\n\\n')\n",
    "# print(sql22.decode()[:print_len] + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bkt = 'd-dat-data-eng-dev'\n",
    "fle = '2020-08-24-26-1.csv'\n",
    "\n",
    "# https://gcsfs.readthedocs.io/en/latest/#credentials doesn't work\n",
    "# df_squarepoint = pd.read_excel(f'gcs://{bkt}/{csv_squarepoint}', storage_options={\"token\": \"anon\"})\n",
    "\n",
    "csv_squarepoint = gcsV.file_get(bkt, fle, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = StringIO(csv_squarepoint.decode())\n",
    "\n",
    "# don't think can be split from StringIO action above - once only stream?\n",
    "df_csv = pd.read_csv(data, nrows=2000, header = [5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from py_modules import bq_helper\n",
    "\n",
    "bqh = bq_helper.bqh('devandtest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prj_ds = 'skytra-benchmark-rnd.products_m'\n",
    "prj_ds = 'skytra-benchmark-devandtest.scratch_PaD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbq cp skytra-benchmark-rnd:products_IN1139.P001_Revenue_per_region_pair skytra-benchmark-devandtest:scratch_PaD.P001_Revenue_per_region_pair\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "No sa permissions to RnD so need to copy to DevTest/other to both a) not save credentials in Jupyter Lab b) use sa credentials/Jupyter\n",
    "\n",
    "bq cp skytra-benchmark-rnd:products_IN1139.P001_Revenue_per_region_pair skytra-benchmark-devandtest:scratch_PaD.P001_Revenue_per_region_pair\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ptbl = 'P0001'\n",
    "Vtbl = f'vw_{Ptbl}_Pivot'\n",
    "\n",
    "sql01_fin = f\"\"\"\n",
    "SELECT dt_of_issue, sum(yoy_rpk_7day) as yoy_rpk_7day\n",
    "FROM {prj_ds}.P001_Revenue_per_region_pair \n",
    "WHERE dt_of_issue >= '2019-01-01' and dt_of_issue <= '2019-12-31' AND region_pair IN ('North America-North America') \n",
    "GROUP BY 1\n",
    "ORDER BY 1\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "sql = sql01_fin\n",
    "csv_cols = ['Product Code', '99NANAWEKALLDRPY']\n",
    "\n",
    "\n",
    "# having manually derived _fin(al) sql from automatically downloaded validation sql\n",
    "#  run the qry against the active GoogleSDK project\n",
    "df01 = bqh.run_qry(sql)\n",
    "# df01.head()\n",
    "\n",
    "# get the equivalent column(s) from the CSV file\n",
    "df_csv_sub01 = df_csv[csv_cols]\n",
    "df_csv_sub01.columns = df_csv_sub01.columns.droplevel()\n",
    "# df_csv_sub01.head()\n",
    "\n",
    "# merge and visually compare prior to auto-compare\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\n",
    "dfm = pd.merge(df01, df_csv_sub01, how = 'outer', left_index = True, right_index = True)\n",
    "dfm['Var'] = dfm.iloc[:,1].astype(float) - dfm.iloc[:,3]\n",
    "dfm['Equal'] = dfm.iloc[:,1].astype(float).eq(dfm.iloc[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT dt_of_issue, sum(yoy_rpk_7day) as yoy_rpk_7day\n",
      "FROM skytra-benchmark-devandtest.scratch_PaD.P001_Revenue_per_region_pair \n",
      "WHERE dt_of_issue >= '2019-01-01' and dt_of_issue <= '2019-12-31' AND region_pair IN ('North America-North America') \n",
      "GROUP BY 1\n",
      "ORDER BY 1\n",
      "\n",
      " **P table P0001** sql to compare to Airtyx CSV columns... \n",
      "\n",
      "['Product Code', '99NANAWEKALLDRPY']\n",
      "\n",
      " **START VARIANCES *************** \n",
      " *********************************\n",
      "Empty DataFrame\n",
      "Columns: [dt_of_issue, yoy_rpk_7day, Product Date, Cumulative volume sold (RPK) - % change YoY, Var, Equal]\n",
      "Index: []\n",
      "\n",
      " **END VARIANCES ***************** \n",
      " *********************************\n",
      "\n",
      " BASIC STATS \n",
      "\n",
      "dt_of_issue.min() = 2019-01-01\n",
      "dt_of_issue.max() = 2019-12-31\n",
      "dt_of_issue                                    365\n",
      "yoy_rpk_7day                                   365\n",
      "Product Date                                   365\n",
      "Cumulative volume sold (RPK) - % change YoY    365\n",
      "Var                                            365\n",
      "Equal                                          365\n",
      "dtype: int64\n",
      "\n",
      " DATAFRAME SUMMARY \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt_of_issue</th>\n",
       "      <th>yoy_rpk_7day</th>\n",
       "      <th>Product Date</th>\n",
       "      <th>Cumulative volume sold (RPK) - % change YoY</th>\n",
       "      <th>Var</th>\n",
       "      <th>Equal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>0.027917679</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>0.027918</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>0.034785132</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>0.034785</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>0.026887988</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>0.026888</td>\n",
       "      <td>3.469447e-18</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>0.029920895</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>0.029921</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-05</td>\n",
       "      <td>-0.007237113</td>\n",
       "      <td>2019-01-05</td>\n",
       "      <td>-0.007237</td>\n",
       "      <td>8.673617e-19</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>0.03891177</td>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>0.038912</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>0.056607976</td>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>0.056608</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>0.055012132</td>\n",
       "      <td>2019-12-29</td>\n",
       "      <td>0.055012</td>\n",
       "      <td>-6.938894e-18</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>0.062972099</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>0.062972</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>0.069628398</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>0.069628</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    dt_of_issue  yoy_rpk_7day Product Date  \\\n",
       "0    2019-01-01   0.027917679   2019-01-01   \n",
       "1    2019-01-02   0.034785132   2019-01-02   \n",
       "2    2019-01-03   0.026887988   2019-01-03   \n",
       "3    2019-01-04   0.029920895   2019-01-04   \n",
       "4    2019-01-05  -0.007237113   2019-01-05   \n",
       "..          ...           ...          ...   \n",
       "360  2019-12-27    0.03891177   2019-12-27   \n",
       "361  2019-12-28   0.056607976   2019-12-28   \n",
       "362  2019-12-29   0.055012132   2019-12-29   \n",
       "363  2019-12-30   0.062972099   2019-12-30   \n",
       "364  2019-12-31   0.069628398   2019-12-31   \n",
       "\n",
       "     Cumulative volume sold (RPK) - % change YoY           Var  Equal  \n",
       "0                                       0.027918  0.000000e+00   True  \n",
       "1                                       0.034785  0.000000e+00   True  \n",
       "2                                       0.026888  3.469447e-18  False  \n",
       "3                                       0.029921  0.000000e+00   True  \n",
       "4                                      -0.007237  8.673617e-19  False  \n",
       "..                                           ...           ...    ...  \n",
       "360                                     0.038912  0.000000e+00   True  \n",
       "361                                     0.056608  0.000000e+00   True  \n",
       "362                                     0.055012 -6.938894e-18  False  \n",
       "363                                     0.062972  0.000000e+00   True  \n",
       "364                                     0.069628  0.000000e+00   True  \n",
       "\n",
       "[365 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ptbl = 'P0001'\n",
    "print(sql + f'\\n **P table {Ptbl}** sql to compare to Airtyx CSV columns... \\n')\n",
    "print(csv_cols)\n",
    "\n",
    "print('\\n **START VARIANCES *************** \\n *********************************')\n",
    "    # dfm[~dfm['Equal']==False & dfm['VAR'].abs() > .001].head(1000)\n",
    "    # TypeError: Cannot perform 'rand_' with a dtyped [float64] array and scalar of type [bool]\n",
    "print(dfm[dfm['Var'].abs() > .00001])\n",
    "print('\\n **END VARIANCES ***************** \\n *********************************')\n",
    "\n",
    "print('\\n BASIC STATS \\n')\n",
    "print( 'dt_of_issue.min() = ' + str(dfm.dt_of_issue.min()) )\n",
    "print( 'dt_of_issue.max() = ' + str(dfm.dt_of_issue.max()) )\n",
    "print(dfm.count())\n",
    "print('\\n DATAFRAME SUMMARY \\n')\n",
    "display(dfm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sql02_fin = f\"\"\"\n",
    "SELECT dt_of_issue, sum(yoy_revenue) as yoy_revenue\n",
    "FROM {prj_ds}.P002_Revenue_yield_per_region_pair_airline \n",
    "WHERE dt_of_issue >= '2019-01-01' and dt_of_issue <= '2019-12-31' AND flight_op_carrier IN ('B6') AND region_pair IN ('Latin America-North America')\n",
    "GROUP BY 1\n",
    "ORDER BY 1\n",
    "\"\"\"\n",
    "\n",
    "Ptbl = 'P0002'\n",
    "sql = sql02_fin\n",
    "csv_cols = ['Product Code', 'B6LANAALLALLDGRY']\n",
    "\n",
    "\n",
    "# having manually derived _fin(al) sql from automatically downloaded validation sql\n",
    "#  run the qry against the active GoogleSDK project\n",
    "df02 = bqh.run_qry(sql)\n",
    "# df02.head()\n",
    "\n",
    "# get the equivalent column(s) from the CSV file\n",
    "df_csv_sub02 = df_csv[csv_cols]\n",
    "df_csv_sub02.columns = df_csv_sub02.columns.droplevel()\n",
    "# df_csv_sub02.head()\n",
    "\n",
    "# merge and visually compare prior to auto-compare\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\n",
    "dfm = pd.merge(df02, df_csv_sub02, how = 'outer', left_index = True, right_index = True)\n",
    "dfm['Var'] = dfm.iloc[:,1].astype(float) - dfm.iloc[:,3]\n",
    "dfm['Equal'] = dfm.iloc[:,1].astype(float).eq(dfm.iloc[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ptbl = 'P0002'\n",
    "print(sql + f'\\n **P table {Ptbl}** sql to compare to Airtyx CSV columns... \\n')\n",
    "print(csv_cols)\n",
    "\n",
    "print('\\n **START VARIANCES *************** \\n *********************************')\n",
    "\n",
    "print(dfm[dfm['Var'].abs() > .00001])\n",
    "print('\\n **END VARIANCES ***************** \\n *********************************')\n",
    "\n",
    "print('\\n BASIC STATS \\n')\n",
    "print( 'dt_of_issue.min() = ' + str(dfm.dt_of_issue.min()) )\n",
    "print( 'dt_of_issue.max() = ' + str(dfm.dt_of_issue.max()) )\n",
    "print(dfm.count())\n",
    "print('\\n DATAFRAME SUMMARY \\n')\n",
    "print(dfm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sql19_fin = f\"\"\"\n",
    "SELECT dt_of_issue, sum(cumulative_yoy_revenue) as cumulative_yoy_revenue \n",
    "FROM {prj_ds}.P019_S_curve_months_per_airline \n",
    "WHERE dt_of_issue >= '2019-01-01' and dt_of_issue <= '2019-12-31' AND op_carrier IN ('UA') AND code_delivery IN ('M092')\n",
    "GROUP BY 1\n",
    "ORDER BY 1\n",
    "\"\"\"\n",
    "\n",
    "Ptbl = 'P0019'\n",
    "sql = sql19_fin\n",
    "csv_cols = ['Product Code', 'UAWWWWALLM092GRY']\n",
    "\n",
    "#  run the qry against the active GoogleSDK project\n",
    "df19 = bqh.run_qry(sql)\n",
    "df19 = df19[df19.cumulative_yoy_revenue.notnull()]\n",
    "\n",
    "# get the equivalent column(s) from the CSV file\n",
    "df_csv_sub19 = df_csv[csv_cols]\n",
    "df_csv_sub19.columns = df_csv_sub19.columns.droplevel()\n",
    "df_csv_sub19.columns = ['product_date', df_csv_sub19.iloc[:,1].name]\n",
    "\n",
    "# convert from object to datetime to allow merge by name - needed because cannot merge by index for Delivery Periods\n",
    "df_csv_sub19.product_date = pd.to_datetime(df_csv_sub19.product_date)\n",
    "\n",
    "# merge \n",
    "dfm = pd.merge(df19, df_csv_sub19, how='inner', on = None, left_on = 'dt_of_issue', right_on = 'product_date')\n",
    "dfm['Var'] = dfm.iloc[:,1].astype(float) - dfm.iloc[:,3]\n",
    "dfm['Equal'] = dfm.iloc[:,1].astype(float).eq(dfm.iloc[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ptbl = 'P0019'\n",
    "print(sql + f'\\n **P table {Ptbl}** sql to compare to Airtyx CSV columns... \\n')\n",
    "print(csv_cols)\n",
    "\n",
    "print('\\n **START VARIANCES *************** \\n *********************************')\n",
    "\n",
    "print(dfm[dfm['Var'].abs() > .00001])\n",
    "print('\\n **END VARIANCES ***************** \\n *********************************')\n",
    "\n",
    "print('\\n BASIC STATS \\n')\n",
    "print( 'dt_of_issue.min() = ' + str(dfm.dt_of_issue.min()) )\n",
    "print( 'dt_of_issue.max() = ' + str(dfm.dt_of_issue.max()) )\n",
    "print(dfm.count())\n",
    "print('\\n DATAFRAME SUMMARY \\n')\n",
    "print(dfm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df20.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sql20_fin = f\"\"\"\n",
    "SELECT dt_of_issue, sum(yoy_yield_7day) as cumulative_yoy_yield_7day \n",
    "FROM {prj_ds}.P020_S_curve_months_per_airline_region \n",
    "WHERE dt_of_issue >= '2020-01-01' and dt_of_issue <= '2020-12-31' AND op_carrier IN ('B6') AND region_pair IN ('Middle East-North America') AND code_delivery IN ('M073')\n",
    "GROUP BY 1\n",
    "ORDER BY 1\n",
    "\"\"\"\n",
    "\n",
    "Ptbl = 'P0020'\n",
    "sql = sql20_fin\n",
    "csv_cols = ['Product Code', 'B6MENAWEKM073YYO']\n",
    "\n",
    "#  run the qry against the active GoogleSDK project\n",
    "df20 = bqh.run_qry(sql)\n",
    "df20 = df20[df20.cumulative_yoy_revenue.notnull()]\n",
    "\n",
    "# get the equivalent column(s) from the CSV file\n",
    "df_csv_sub20 = df_csv[csv_cols]\n",
    "df_csv_sub20.columns = df_csv_sub20.columns.droplevel()\n",
    "df_csv_sub20.columns = ['product_date', df_csv_sub20.iloc[:,1].name]\n",
    "\n",
    "# convert from object to datetime to allow merge by name - needed because cannot merge by index for Delivery Periods\n",
    "df_csv_sub20.product_date = pd.to_datetime(df_csv_sub20.product_date)\n",
    "\n",
    "# merge \n",
    "dfm = pd.merge(df20, df_csv_sub20, how='inner', on = None, left_on = 'dt_of_issue', right_on = 'product_date')\n",
    "dfm['Var'] = dfm.iloc[:,1].astype(float) - dfm.iloc[:,3]\n",
    "dfm['Equal'] = dfm.iloc[:,1].astype(float).eq(dfm.iloc[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ptbl = 'P0020'\n",
    "print(sql + f'\\n **P table {Ptbl}** sql to compare to Airtyx CSV columns... \\n')\n",
    "print(csv_cols)\n",
    "\n",
    "print('\\n **START VARIANCES *************** \\n *********************************')\n",
    "\n",
    "print(dfm[dfm['Var'].abs() > .00001])\n",
    "print('\\n **END VARIANCES ***************** \\n *********************************')\n",
    "\n",
    "print('\\n BASIC STATS \\n')\n",
    "print( 'dt_of_issue.min() = ' + str(dfm.dt_of_issue.min()) )\n",
    "print( 'dt_of_issue.max() = ' + str(dfm.dt_of_issue.max()) )\n",
    "print(dfm.count())\n",
    "print('\\n DATAFRAME SUMMARY \\n')\n",
    "print(dfm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sql21_fin = f\"\"\"\n",
    "SELECT dt_of_issue, sum(cumulative_yoy_yield) as cumulative_yoy_yield\n",
    "FROM {prj_ds}.P021_S_curve_months_per_region \n",
    "WHERE dt_of_issue >= '2019-01-01' and dt_of_issue <= '2019-12-31' AND region_pair IN ('North America-North America') AND code_delivery IN ('M073') \n",
    "GROUP BY 1\n",
    "ORDER BY 1\n",
    "\"\"\"\n",
    "\n",
    "Ptbl = 'P0021'\n",
    "sql = sql21_fin\n",
    "csv_cols = ['Product Code', '99NANAALLM073YYO']\n",
    "\n",
    "#  run the qry against the active GoogleSDK project\n",
    "df21 = bqh.run_qry(sql)\n",
    "df21 = df21[df21.iloc[:,1].notnull()]\n",
    "df21.dt_of_issue = pd.to_datetime(df21.dt_of_issue)\n",
    "\n",
    "# get the equivalent column(s) from the CSV file\n",
    "df_csv_sub21 = df_csv[csv_cols]\n",
    "df_csv_sub21.columns = df_csv_sub21.columns.droplevel()\n",
    "df_csv_sub21.columns = ['product_date', df_csv_sub21.iloc[:,1].name]\n",
    "\n",
    "# convert from object to datetime to allow merge by name - needed because cannot merge by index for Delivery Periods\n",
    "# df_csv_sub21.product_date = pd.to_datetime(df_csv_sub21.product_date)\n",
    "\n",
    "# merge \n",
    "dfm = pd.merge(df21, df_csv_sub21, how='inner', on = None, left_on = 'dt_of_issue', right_on = 'product_date')\n",
    "dfm['Var'] = dfm.iloc[:,1].astype(float) - dfm.iloc[:,3]\n",
    "dfm['Equal'] = dfm.iloc[:,1].astype(float).eq(dfm.iloc[:,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ptbl = 'P0021'\n",
    "print(sql + f'\\n **P table {Ptbl}** sql to compare to Airtyx CSV columns... \\n')\n",
    "print(csv_cols)\n",
    "\n",
    "print('\\n **START VARIANCES *************** \\n *********************************')\n",
    "\n",
    "print(dfm[dfm['Var'].abs() > .00001])\n",
    "print('\\n **END VARIANCES ***************** \\n *********************************')\n",
    "\n",
    "print('\\n BASIC STATS \\n')\n",
    "print( 'dt_of_issue.min() = ' + str(dfm.dt_of_issue.min()) )\n",
    "print( 'dt_of_issue.max() = ' + str(dfm.dt_of_issue.max()) )\n",
    "print(dfm.count())\n",
    "print('\\n DATAFRAME SUMMARY \\n')\n",
    "print(dfm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sql22_fin = f\"\"\"\n",
    "SELECT dt_of_issue, sum(yoy_net_revenue_7day) as yoy_net_revenue_7day  \n",
    "FROM {prj_ds}.P022_Revenue_per_airline \n",
    "WHERE dt_of_issue >= '2019-01-01' and dt_of_issue <= '2019-12-31' AND op_carrier IN ('DL') GROUP BY 1\n",
    "ORDER BY 1\n",
    "\"\"\"\n",
    "\n",
    "Ptbl = 'P0022'\n",
    "sql = sql22_fin\n",
    "csv_cols = ['Product Code', 'DLWWWWWEKALLDNRY']\n",
    "\n",
    "#  run the qry against the active GoogleSDK project\n",
    "df22 = bqh.run_qry(sql)\n",
    "# .notnull() filter not needed because no Delivery Month\n",
    "df22.dt_of_issue = pd.to_datetime(df22.dt_of_issue)\n",
    "\n",
    "# get the equivalent column(s) from the CSV file\n",
    "df_csv_sub22 = df_csv[csv_cols]\n",
    "df_csv_sub22.columns = df_csv_sub22.columns.droplevel()\n",
    "df_csv_sub22.columns = ['product_date', df_csv_sub22.iloc[:,1].name]\n",
    "\n",
    "# error A value is trying to be set on a copy of a slice from a DataFrame.\n",
    "# HOWEVER, bogus error, works anyway, code_kata nb on personal laptop has the method to switch this off\n",
    "df_csv_sub22.product_date = pd.to_datetime(df_csv_sub22.product_date)\n",
    "\n",
    "# merge could be done using index because no Delivery Month, but in case outer join issues, preferable to join by date\n",
    "dfm = pd.merge(df22, df_csv_sub22, how='outer', on = None, left_on = 'dt_of_issue', right_on = 'product_date')\n",
    "dfm['Var'] = dfm.iloc[:,1].astype(float) - dfm.iloc[:,3]\n",
    "dfm['Equal'] = dfm.iloc[:,1].astype(float).eq(dfm.iloc[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfm = pd.merge(df22, df_csv_sub22, how='outer', on = None, left_on = 'dt_of_issue', right_on = 'product_date')\n",
    "dfm['Var'] = dfm.iloc[:,1].astype(float) - dfm.iloc[:,3]\n",
    "dfm['Equal'] = dfm.iloc[:,1].astype(float).eq(dfm.iloc[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ptbl = 'P0022'\n",
    "print(sql + f'\\n **P table {Ptbl}** sql to compare to Airtyx CSV columns... \\n')\n",
    "print(csv_cols)\n",
    "\n",
    "print('\\n **START VARIANCES *************** \\n *********************************')\n",
    "\n",
    "print(dfm[dfm['Var'].abs() > .00001])\n",
    "print('\\n **END VARIANCES ***************** \\n *********************************')\n",
    "\n",
    "print('\\n BASIC STATS \\n')\n",
    "print( 'dt_of_issue.min() = ' + str(dfm.dt_of_issue.min()) )\n",
    "print( 'dt_of_issue.max() = ' + str(dfm.dt_of_issue.max()) )\n",
    "print(dfm.count())\n",
    "print('\\n DATAFRAME SUMMARY \\n')\n",
    "print(dfm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
